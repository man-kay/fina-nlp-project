{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>transcript_esg</th>\n",
       "      <th>esg_score</th>\n",
       "      <th>esg_risk_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>thank emily welcome everyone agilents conferen...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>thank hannah welcome everyone agilents confere...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>good day welcome apple q fy earnings conferenc...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>good day welcome apple q fy earnings conferenc...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>good day welcome apple q fy earnings conferenc...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>WMT</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>followed questionandanswer session turn call d...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>YUM</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>welcome q yum brand earnings conference call n...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>YUM</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>get started would like remind conference call ...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>thank operator good morning everyone welcome z...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>ZTS</td>\n",
       "      <td>2022</td>\n",
       "      <td>2</td>\n",
       "      <td>thank good morning everyone welcome zoetis sec...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>547 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    symbol  year  quarter                                     transcript_esg  \\\n",
       "0        A  2022        1  thank emily welcome everyone agilents conferen...   \n",
       "1        A  2022        3  thank hannah welcome everyone agilents confere...   \n",
       "2     AAPL  2022        1  good day welcome apple q fy earnings conferenc...   \n",
       "3     AAPL  2022        2  good day welcome apple q fy earnings conferenc...   \n",
       "4     AAPL  2022        3  good day welcome apple q fy earnings conferenc...   \n",
       "..     ...   ...      ...                                                ...   \n",
       "542    WMT  2023        3  followed questionandanswer session turn call d...   \n",
       "543    YUM  2022        1  welcome q yum brand earnings conference call n...   \n",
       "544    YUM  2022        2  get started would like remind conference call ...   \n",
       "545    ZTS  2022        1  thank operator good morning everyone welcome z...   \n",
       "546    ZTS  2022        2  thank good morning everyone welcome zoetis sec...   \n",
       "\n",
       "     esg_score esg_risk_level  \n",
       "0         15.0            Low  \n",
       "1         15.0            Low  \n",
       "2         17.0            Low  \n",
       "3         17.0            Low  \n",
       "4         17.0            Low  \n",
       "..         ...            ...  \n",
       "542       25.0         Medium  \n",
       "543       21.0         Medium  \n",
       "544       21.0         Medium  \n",
       "545       18.0            Low  \n",
       "546       18.0            Low  \n",
       "\n",
       "[547 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../../data/processed/train_esg_shortened.csv')\n",
    "test = pd.read_csv('../../data/processed/test_esg_shortened.csv')\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique ESG risk levels: 5\n",
      "Unique classes: ['Low' 'Medium' 'Severe' 'High' 'Negligible']\n"
     ]
    }
   ],
   "source": [
    "# Find the number of unique classes in the 'esg_risk_level' column\n",
    "unique_classes = train['esg_risk_level'].unique()\n",
    "num_classes = len(unique_classes)\n",
    "\n",
    "print(f\"Number of unique ESG risk levels: {num_classes}\")\n",
    "print(f\"Unique classes: {unique_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tokenizer and encode texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and encode texts\n",
    "tokenizer = AutoTokenizer.from_pretrained('nbroad/ESG-BERT')\n",
    "def encode_texts(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode and handle labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "# Encode and handle labels\n",
    "label_encoder = LabelEncoder()\n",
    "train['esg_risk_level'] = label_encoder.fit_transform(train['esg_risk_level'])\n",
    "test['esg_risk_level'] = label_encoder.transform(test['esg_risk_level'])\n",
    "train['transcript_esg'].fillna('', inplace=True)\n",
    "test['transcript_esg'].fillna('', inplace=True)\n",
    "train_encodings = encode_texts(train['transcript_esg'].tolist())\n",
    "test_encodings = encode_texts(test['transcript_esg'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESGDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)  # Ensure labels are long from the start\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ESGDataset(train_encodings, train['esg_risk_level'].tolist())\n",
    "test_dataset = ESGDataset(test_encodings, test['esg_risk_level'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup with class weights to handle imbalance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nbroad/ESG-BERT and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([26]) in the checkpoint and torch.Size([5]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([26, 768]) in the checkpoint and torch.Size([5, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(label_encoder.classes_)\n",
    "class_weights = torch.tensor([1.0 / train['esg_risk_level'].value_counts()[i] for i in range(num_classes)], dtype=torch.float)  # Explicitly define as float\n",
    "class_weights = class_weights / class_weights.sum() * num_classes\n",
    "model = BertForSequenceClassification.from_pretrained('nbroad/ESG-BERT', num_labels=num_classes, ignore_mismatched_sizes=True)\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, num_classes)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights.to(torch.device(\"cpu\")))  # Ensure weights are on the correct device and float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "device = torch.device(\"cpu\")  # Use \"cuda\" if you have GPU\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 35/35 [08:51<00:00, 15.18s/it]\n",
      "Training Epoch 2: 100%|██████████| 35/35 [08:52<00:00, 15.21s/it]\n",
      "Training Epoch 3: 100%|██████████| 35/35 [09:16<00:00, 15.89s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Loop\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits.float(), labels)  # Ensure logits are FloatTensor\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_and_save_results(model, dataset, data, filename):\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, batch_size=16)\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            predictions = outputs.logits.argmax(dim=-1).cpu().numpy()\n",
    "            all_predictions.extend(predictions)\n",
    "\n",
    "    # Convert numerical predictions back to label names using the inverse transformation of the label encoder\n",
    "    predicted_labels = label_encoder.inverse_transform(all_predictions)\n",
    "\n",
    "    # Append predictions to the DataFrame\n",
    "    data['predicted_esg_risk_level'] = predicted_labels\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(data['esg_risk_level'], all_predictions)\n",
    "    print(f\"Accuracy for {filename}: {accuracy}\")\n",
    "\n",
    "    # Save to CSV\n",
    "    data.to_csv(filename, index=False)\n",
    "    print(f\"Results saved to {filename}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for ../../data/processed/train_results.csv: 0.5137111517367459\n",
      "Results saved to ../../data/processed/train_results.csv\n",
      "Accuracy for ../../data/processed/test_results.csv: 0.463768115942029\n",
      "Results saved to ../../data/processed/test_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n",
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/owenwong/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "# Use the modified function for both train and test datasets\n",
    "train_accuracy = evaluate_and_save_results(model, train_dataset, train.copy(), '../../data/processed/train_results.csv')\n",
    "test_accuracy = evaluate_and_save_results(model, test_dataset, test.copy(), '../../data/processed/test_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_path = \"../../model/trained_ESG_BERT.pth\"\n",
    "torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
